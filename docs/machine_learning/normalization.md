!!! warning
    This page is still ongoing modifications. Please check back after some time or [contact me](mailto:mohitmayank1@gmail.com) if it has been a while! Sorry for the inconvinence :pray:

## Introduction

- In Deep Learning, Normalization is the process of providing uniform scale for numerical value. In Machine learning, normalzation is performed only once at the time of data preprocessing, but in Deep Learning it is applied at multiple places and hence multiple times during training and inference.
- This is because even if we normalize the input data, the presence of multiple hidden layers and non-linear activations could lead to different distribution of data at different layers. This could lead to instability in training, as loosly speaking, now model will have to spend additional effort in handling the different distributions. 

## Types

- Several normalization layers were introduced that helps with reducing the distribution shift problem (*internal covariance shift*). We will go through them next.

### Batch Normalization

### Layer Normalization

### Weigth Normalization

### Instance Normalization

### Group Normalization